---
title: "Creating Alerts using Prometheus metrics in AlertManager"
date: 2025-09-06 19:00
description: "Alerting within a system is extremely important to know when things are going wrong. Learn how to create your own alerts with Prometheus Alertmanager."
path: "prometheus"
tags:
- Prometheus
- Observability
---
# Alerting Rules

Prometheus server can trigger and create alerts when particular metrics have unexpected values.
You may have noticed the "Alerts" tab within the Prometheus GUI.
This tab will display all of the alerts in the server, however, it does require some configuration.

To show alerts, we will need to setup some [Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/).
Once we have setup these alerting rules, Prometheus itself cannot do much with these values other than show them in the web browser.
In order to take action on particular alerts or ship the alerts off to Grafana, we will need to add in the Prometheus AlertManager.

First, let's configure some alerts.
[In Lesson 3](./3-promql.md) we learnt about some PromQL queries that we could make using our metrics.
Let's create an alert for this metric, specifically, let's look at this query:

```
sum(sample_sampleClientApp_calculation_value)
```

![Sum All](./images/prometheus/SumAllLabels.png)

In this image, you can see that our "sum" has swings from being above 0 and has some values below zero (your application may vary!).
Let's say that anything under 0 here is an error that we want reported!

What we can do is setup an alerting rule for this metric. Using the documentation on Alerting rules linked above, we can setup an alerting rule as follows:

> To include rules in Prometheus, create a file containing the necessary rule statements and have Prometheus load the file via the rule_files field in the Prometheus configuration. Rule files use YAML.

First, let's set the `rule_files` in the `pprometheus.yml`:

```yaml
rule_files:
  - alerting_rules.yml
```

Now, let's create a new file called `alerting_rules.yml` and configure our alert:

```yaml
groups:
  - name: SampleAlerts
    rules:
        # The name of the alert. Must be a valid label value.
      - alert: CalculationBelowZero
        # The PromQL expression to evaluate. Every evaluation cycle this is
        # evaluated at the current time, and all resultant time series become
        # pending/firing alerts.
        expr: sum(sample_sampleClientApp_calculation_value) < 0
        # Alerts are considered firing once they have been returned for this long.
        # Alerts which have not yet fired for long enough are considered pending.
        for: 0s
        # Labels to add or overwrite for each alert.
        # Here we are adding a "severity" label so we can create a filter to see only values with a particular severity
        labels:
          severity: high
        # Annotations to add to each alert.
        # These are descriptions for the alert.
        # Can use template strings to substitute values
        annotations:
          summary: Sum of returned values is below zero!
```

Once we have configured our alert, let's stop and restart the Prometheus docker container.

NOTE: Stopping the Prometheus server will clear out all of our old data.
Therefore, depending on the data generated by our app, it is possible that you many not see the alert firing (ever!).

Going back into the Prometheus GUI, we can now click the GUI's "Alerts" tab and be shown a list of all of the configured alerts!

![Alert](./images/prometheus/ConfiguredAlert.png)

Here we can click on the expression to view the data returned by the expression. However, some of you may see this:

![No Data Alert](./images/prometheus/NoDataAlert.png)

Why? This is because of our query. Our query is `sum(sample_sampleClientApp_calculation_value) < 0` which is searching for a metric of values less than zero.
This query will ONLY show values under zero and, if there are none, will not show any data at all.
To ensure that we are always triggering the alert, we could change our app to always be negative.

Navigate to `UnitOfWorkNoPrometheusImpl` and make the `shouldBeNegative` variable always true.
Then, stop and restart our local application.

After restarting, wait for prometheus to do an initial scrape of our application, and, we will see that our alert is firing!

NOTE: In the server config, we have a configuration called `evaluation_interval`. This is how often alerting rules are evaluated to determine if they are firing or not.
The default is 1 minute, so you will need to wait at least 1 minute before you see the alert firing.


![Active Alert](./images/prometheus/ActiveAlert.png)


Amazing! You have successfully configured an alert in Prometheus Server!
But... what can you do with this alert?

# Prometheus Alertmanager

Prometheus AlertManager is a second service that runs in tandem with Prometheus server.
The job of AlertManager is to recieve prometheus alerts in order to perform actions based on firing alerts.
For example: Send emails, ping slack channels, trigger HTTP webhooks, broadcast to Grafana, etc.

[Since Grafana 8, Grafana supports Alerting](https://grafana.com/docs/grafana/latest/alerting/).
Prometheus Alertmanager works directly with Grafana's alerting system which allows you to easilt broadcast and raise alerts from prometheus directly to Grafana.

To begin using Prometheus AlertManager, we need to do three things:


1. Configure alerting rules (Done!)
2. Make the Prometheus server aware that we have an alert manager
3. Configure Alertmanager broadcasting
4. Start alert manager

### Tell Prometheus about Alertmanager

In order for the prometheus server to know it should report metrics to the alert manager, we need to change our configuration for the prometheus server.
Navigate to your Prometheus config file and add the following configuration:

```yaml
alerting:
  alertmanagers:
    - scheme: http
      static_configs:
        # This is a list of alertmanager targets. Change the port/IP based on your configuration.
        # For this demo, localhost:9093 will work.
        - targets: [ 'localhost:9093' ]
```

Once you have setup this configuration in Prometheus server, restart the Prometheus server docker container.

### Configure Prometheus Alertmanager

In order for Prometheus Alertmanager to send out and broadcast notifications when an alert happens, it needs to be configured.
[The documentation here](https://prometheus.io/docs/alerting/latest/configuration/) shows all of the ways you can configure alert manager to broadcast when alerts are firing.

Note: We will not configure anything because, running locally, we have nothing to broadcast to.

Also Note: You don't need to configure Alertmanager to "broadcast" to Grafana, simply add Alertmanager as a datasource in Grafana and everything works perfectly. 

### Start Prometheus AlertManager

Again, there are many ways you can do this, but locally will can this with docker.

Run the following command to run AlertManager:

(This is just an example for how to do this. Since we have nothing to broadcast to, you don't need to run this unless you really want to.)

```shell
docker run \
    --network host \
    -v ${PWD}/config:/config \
    prom/alertmanager
    --config.file=/config/alertmanager.yml --log.level=debug
```

Voila! You have started the prometheus alert manager.

For us, there is not much more we can do since we are just running things locally.
We don't have slack credentials, or an email server, or a grafana instance to show off how prometheus alertmanager can send out alerts to external systems.
However, this has shown you the basics of setting up AlertManager and gives you enough information to use it in the future if you chose.

That is all you need to know about Prometheus!

Prometheus is an extremely powerful tool and I recommend that any software system should make use of prometheus for observability.
